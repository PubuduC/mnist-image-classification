# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1el6QnkBZ6VyBj84F2WGF02On-O6_tEHR
"""

from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2D, Dropout,Flatten, BatchNormalization
from keras.models import Model, Sequential
from keras.datasets import mnist
from keras.utils import to_categorical
from keras.optimizers import SGD
from statistics import *
import numpy as np

# The encoding process
input_img = Input(shape=(28, 28, 1))  


# Conv1 #
#convolution layer and relu layer
x = Conv2D(filters = 16, kernel_size = (3, 3), activation='relu', padding='same')(input_img)
#pooling layer
x = MaxPooling2D(pool_size = (2, 2), padding='same')(x)

# Conv2 #
#convolution layer and relu layer
x = Conv2D(filters = 8, kernel_size = (3, 3), activation='relu', padding='same')(x)
#pooling layer
x = MaxPooling2D(pool_size = (2, 2), padding='same')(x) 

# Conv 3 #
#convolution layer and relu layer
x = Conv2D(filters = 8, kernel_size = (3, 3), activation='relu', padding='same')(x)
#pooling layer
encoded = MaxPooling2D(pool_size = (2, 2), padding='same')(x)

# DeConv1
#convolution layer and relu layer
x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)
#The UpSampling2D Layer does the exact opposite of MaxPooling2D
x = UpSampling2D((2, 2))(x)

# DeConv2
#convolution layer and relu layer
x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
#The UpSampling2D Layer does the exact opposite of MaxPooling2D
x = UpSampling2D((2, 2))(x)

# Deconv3
#convolution layer and relu layer
x = Conv2D(16, (3, 3), activation='relu')(x)
#The UpSampling2D Layer does the exact opposite of MaxPooling2D
x = UpSampling2D((2, 2))(x)
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

# Declare the model
autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = mnist.load_data()
    # reshape dataset to have a single channel
    #the pixel values are grayscale, the pixel dimension is set to 1.
    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
    testX = testX.reshape((testX.shape[0], 28, 28, 1))
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY

trainX, trainY, testX, testY = load_dataset()
input_shape = (28, 28, 1)

# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    # Making sure that the values are float so that we can get decimal points after division
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    # Normalizing the grey scale codes by dividing it by the max grey scale value.
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm

trainX, testX = prep_pixels(trainX, testX)

#adding random noise to the data
def generate_noise_data(noise_factor):
    x_train_noisy = trainX+ noise_factor * np.random.normal(loc=0.0, scale=1.0, size=trainX.shape)
    x_test_noisy = testX + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=testX.shape)
    x_train_noisy = np.clip(x_train_noisy, 0., 1.)
    x_test_noisy = np.clip(x_test_noisy, 0., 1.)
    return x_train_noisy, x_test_noisy

noise_factor = 0.50
x_train_noisy_new, x_test_noisy_new = generate_noise_data(noise_factor)

# Train the model
autoencoder.fit(x_train_noisy_new, trainX,
                epochs=10,
                batch_size=128,
                shuffle=True,
                validation_data=(x_test_noisy_new, testX)
               )

import matplotlib.pyplot as plt
decoded_imgs = autoencoder.predict(x_test_noisy_new)

n = 10

plt.figure(figsize=(20, 4))
for i in range(n):
    # display original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test_noisy_new[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # display reconstruction
    ax = plt.subplot(2, n, i+1+n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

decoded_imgs.shape

"""New model is trained using noise reduced decoded images for the classification and it's accuarcies can be seen as follows after each epoch"""

# deep learning models are created where an instance of the Sequential class is created
# model layers are created and added to it.
model = Sequential() 

# 28 filters, each being size of 3x3 and slides through the image  with a stride of magnitude 1 in horizontal and vertical direction using ReLU activation function
model.add(Conv2D(28, kernel_size=(3,3), strides=(1,1), input_shape=input_shape)) 

#adding batch normalization to changing the distribution by standarising the output
model.add(BatchNormalization())

#max pooling to extract digit features
model.add(MaxPooling2D(pool_size=(2,2)))

#increasing the model depth
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', strides=(1,1)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', strides=(1,1)))
model.add(MaxPooling2D(pool_size=(2, 2)))
#turns matrix values into a single vector
model.add(Flatten()) 
#hidden layer with 200 nodes and relu activation function
model.add(Dense(200,activation = "relu")) 
# dropout regularization to drop certain neurons to reduce overfitting
model.add(Dropout(0.3)) 
#adding batch normalization to changing the distribution by standarising the output
model.add(BatchNormalization())
#output layer for classification values of digits 0-9
model.add(Dense(10,activation = "softmax"))
# compile model
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(decoded_imgs, testY, epochs=10, validation_split=0.1)

